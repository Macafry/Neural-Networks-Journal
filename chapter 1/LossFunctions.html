<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lossfunctions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="LossFunctions_files/libs/clipboard/clipboard.min.js"></script>
<script src="LossFunctions_files/libs/quarto-html/quarto.js"></script>
<script src="LossFunctions_files/libs/quarto-html/popper.min.js"></script>
<script src="LossFunctions_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="LossFunctions_files/libs/quarto-html/anchor.min.js"></script>
<link href="LossFunctions_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="LossFunctions_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="LossFunctions_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="LossFunctions_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="LossFunctions_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="loss-functions" class="level1">
<h1>Loss Functions</h1>
<p>When training a machine learning model, we need a way to measure how well our model is performing. This is where loss functions, denoted as <span class="math inline">\(\operatorname{L_{type}}\)</span>, come in. <strong>Loss functions</strong> are used to quantify the difference between the predicted output of a model and the actual output. The goal of training a machine learning model is to minimize the loss function, which means making the model’s predictions as close as possible to the actual output. Depending on the type of problem we are trying to solve, we can choose different loss functions. For example, if we are trying to classify images, we might use a cross-entropy loss function. If we are trying to predict a continuous value, we might use a mean squared error loss function. This section explores some of the most commonly used loss functions in machine learning.</p>
<p>However, one thing to consider is that the loss function is typically composed by several individal losses, each one corresponding to a different sample. There are two principal ways to aggregate these losses into a single value: averaging them or adding them up. The choice of aggregation method depends on the specific problem and the desired behavior of the model. However, in general, averaging is more common and is typically the default choice. But, for simplicity sake, we will focus on the addition aggregation when defining loss functions.</p>
<section id="regression---mean-squared-error-mse" class="level2">
<h2 class="anchored" data-anchor-id="regression---mean-squared-error-mse">Regression - Mean Squared Error (MSE)</h2>
<p>Mean Squared Error (MSE) is the quintessential loss function for regression problems. It measures the average squared difference between the predicted values and the actual values. The most common formula for MSE is:</p>
<p><span class="math display">\[
\operatorname{L_{MSE}}(y, \hat{y}) = \frac{1}{2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span></p>
<p>There are other loss functions that are used in regression problems, such as Mean Absolute Error (MAE) and R-squared. However, MSE is the most commonly used loss function for regression problems because it is easy to understand and compute, and it has desirable statistical properties, such as being (easily) differentiable and convex.</p>
</section>
<section id="classification---count-based-losses" class="level2">
<h2 class="anchored" data-anchor-id="classification---count-based-losses">Classification - Count based losses</h2>
<section id="confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h4>
<p>While not a loss function itself, a <strong>confusion matrix</strong> (CM) is a table that is often used to describe the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives for a given set of predictions.</p>
<p>From this matrix, we can calculate other metrics such as accuracy, precision, recall, and F1 score. The columns of a confusion matrix represent the predicted classes, while the rows represent the actual classes.</p>
<p>Consider the following example of confusion matrix:</p>
<table class="table">
<thead>
<tr class="header">
<th>class</th>
<th>Blue</th>
<th>Green</th>
<th>Red</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Blue</td>
<td>100</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td>Green</td>
<td>5</td>
<td>95</td>
<td>8</td>
</tr>
<tr class="odd">
<td>Red</td>
<td>10</td>
<td>7</td>
<td>80</td>
</tr>
</tbody>
</table>
<p>Can observe that the diagonal elements represent the number of correct predictions for each class, while other elements represent the number of incorrect predictions (the actual class is on the row and the predicted class is on the column).</p>
</section>
<section id="accuracy" class="level4">
<h4 class="anchored" data-anchor-id="accuracy">Accuracy</h4>
<p>The easiest derived metric is <strong>accuracy</strong>, which is the number of correct predictions over the total number of predictions, or the proportion of correct predictions. We can calculate it as follows:</p>
<p><span class="math display">\[
\operatorname{Accuracy}=\dfrac{\text { Number of correct predictions }}{\text { Total number of predictions }} = \dfrac{\operatorname{trace}(CM)}{N}
\]</span></p>
<p>However, often Confusion Matrix-derived metrics are not suitable to train a machine learning model for one key reason: they are not differentiable. Later sections will explain why we might want loss to be differentiable. Nevertheless, we can still use Confusion Matrix-derived metrics to evaluate and <em>interpret</em> the performance of a trained model.</p>
</section>
</section>
<section id="entropy" class="level2">
<h2 class="anchored" data-anchor-id="entropy">Entropy</h2>
<p><strong>Entropy</strong> is a measure of uncertainty or randomness. It has several derived uses in the context of machine learning, some of which are loss functions. The entropy of a probability distribution, <span class="math inline">\(p\)</span>, is defined as follows:</p>
<p><span class="math display">\[
H(p)=−\sum_{i=1}^{n} p_{i} \log p_{i}
\]</span></p>
<p>It can be interpreted as the “surprise” of a distribution. For example, if we have a fair coin flip, the entropy is 1, because there are two possible outcomes, each with a 50% probability. If we have a biased coin flip, where heads has a 90% probability and tails has a 10% probability, the entropy is lower, because the outcome is less surprising.</p>
<p>In the context of machine learning, <span class="math inline">\(p\)</span> is often a probability vector of <span class="math inline">\(K\)</span> classes. As a side note, maximum entropy for <span class="math inline">\(p\)</span> happens at a uniform distribution, i.e., <span class="math inline">\(p_i = \frac{1}{k}\)</span>, with <span class="math inline">\(H(p) = \log(k)\)</span>.</p>
<section id="cross-entropy-ce" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-ce">Cross-Entropy (CE)</h4>
<p><strong>Cross-entropy</strong> is an asymmetric measure of the difference between two probability distributions. It can be interpreted as the “surprise” of a using distribution <span class="math inline">\(q\)</span> (predicted), when trying to describe a distribution <span class="math inline">\(p\)</span> (true). The cross-entropy between two probability distributions, <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, is defined as follows:</p>
<p><span class="math display">\[
H(p, q)=−\sum_{i=1}^{n} p_{i} \log q_{i}
\]</span></p>
<p>We can use cross-entropy as a loss function for a classsification problem. In this case, <span class="math inline">\(y_i\)</span> is the true distribution vector, and <span class="math inline">\(\hat{y}_i\)</span> is the predicted distribution vector. Then, the cross-entropy loss function is defined as follows:</p>
<p><span class="math display">\[
\operatorname{L_{CE}}(y, \hat{y})=-\sum_{i=1}^{n} y_{i} \log \hat{y}_{i}
\]</span></p>
</section>
<section id="binary-cross-entropy-bce" class="level4">
<h4 class="anchored" data-anchor-id="binary-cross-entropy-bce">Binary Cross-Entropy (BCE)</h4>
<p><strong>Binary cross-entropy</strong> is a special case of cross-entropy for binary classification problems. In these cases, a predicted probability is enough to represent the distribution, so the predicted distribution vector <span class="math inline">\(\hat{y}_i\)</span> is a single value between 0 and 1. The binary cross-entropy loss function is defined as follows: <span class="math display">\[
\operatorname{L_{BCE}}(y, \hat{y})= -\sum_{i=1}^{n} \Big( y_{i} \log \hat{y}_{i} + (1-y_{i}) \log (1-\hat{y}_{i}) \Big)
\]</span></p>
<p>However, this can also be used for several simultaneous binary classification problems, in which case the predicted distribution vector <span class="math inline">\(\hat{y}_i\)</span> is a vector of <span class="math inline">\(k\)</span> probabilities, and the loss function is defined as follows:</p>
<p><span class="math display">\[
\operatorname{L_{BCE}}(y, \hat{y})= -\sum_{i=1}^{n} \sum_{j=1}^{k} \Big( y_{i j} \log \hat{y}_{i j} + (1-y_{i j}) \log (1-\hat{y}_{i j}) \Big)
\]</span></p>
<p>One must only use BCE loss when dealing with true binary values and not just values between 0 and 1. Using BCE loss with values between 0 and 1 will likely result in a case of exploding gradients, which will be explained in a later section. For now, trust that we dont want exploding gradients.</p>
</section>
</section>
<section id="multi-objective-loss-functions---regularization" class="level2">
<h2 class="anchored" data-anchor-id="multi-objective-loss-functions---regularization">Multi-Objective Loss Functions - Regularization</h2>
<p>A loss function is not just a measure of how well a model is performing, it is also a measure of how well the model is generalizing. This is why we have regularization loss functions. Regularization loss functions are used to penalize the model for overfitting.</p>
<p>To create a loss function with regularization, we simply add a (scaled) regularization term to the base loss function. The scaling factor is called the regularization parameter, and it is denoted by <span class="math inline">\(\lambda \ge 0\)</span>. The regularization parameter is a hyperparameter that must be tuned. A general form of a loss function with regularization is:</p>
<p><span class="math display">\[
L(y, \hat{y}) = L_{\text{base}}(y, \hat{y}) + \lambda C(w)
\]</span></p>
<p>Where <span class="math inline">\(R(\theta)\)</span> is the regularization term, and <span class="math inline">\(\theta\)</span> is the model parameters. The regularization term is usually a function of the model parameters, and it is used to penalize the model for having large parameters. Next we will look at some common regularization loss functions in machine learning.</p>
<section id="l2-regularization" class="level4">
<h4 class="anchored" data-anchor-id="l2-regularization">L2 Regularization</h4>
<p><strong>L2 Regularization</strong> is the most popular regularization technique. In the context of regression, it is also known as <strong>Ridge Regression</strong>. It penalizes large parameter values and it behaves very similarly to the MSE loss. One of the reasons its easy to compute derivative.</p>
<p>The L2 regularization term is defined as follows:</p>
<p><span class="math display">\[
C(w) = \frac{1}{2} \sum_{i=1}^{n} w^2
\]</span></p>
<p>There are other Lp regularization techniques, but L2 is the most popular one. The second most popular one is L1 regularization, which is also known as <strong>Lasso Regression</strong> in the context of regression. It also penalizes larger terms, but may also perform feature selection. We won’t use it in this course, but the formula goes as follows:</p>
<p><span class="math display">\[
C(w) = \sum_{i=1}^{n} |w|
\]</span></p>
<p>In the context of neural networks, L2 regularization is also known as <strong>weight decay</strong>.</p>
</section>
<section id="kl-divergence" class="level4">
<h4 class="anchored" data-anchor-id="kl-divergence">KL-Divergence</h4>
<p><strong>KL-Divergence</strong> is a measure of how one probability distribution is different from a second, reference probability distribution. It is useful when we want to <em>force</em> a value to follow a specific distribution. It can be interpreted as the added “surprise” of a using distribution <span class="math inline">\(q\)</span> (predicted), when trying to describe a distribution <span class="math inline">\(p\)</span> (true). It is defined as follows:</p>
<p><span class="math display">\[
D_{KL}(p||q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}
\]</span></p>
<p>It can also be shown that:</p>
<p><span class="math display">\[
D_{KL}(p||q) = H(p,q) - H(p)
\]</span></p>
<p>Where <span class="math inline">\(H(p,q)\)</span> is the <strong>Cross-Entropy</strong> and <span class="math inline">\(H(p)\)</span> is the <strong>Entropy</strong> of the distribution <span class="math inline">\(p\)</span>. Hence the addaed surprise interpretation.</p>
<p>This regularization is typically computed in terms of distribution parameters. For instance, if we fix <span class="math inline">\(q\)</span> to be a standard normal distribution, and <span class="math inline">\(p\)</span> is a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span>, then by following the derivation from <a href="https://statproofbook.github.io/P/norm-kl.html">The Book of Statistical Proofs</a>, we get the following formula for the KL-Divergence:</p>
<p><span class="math display">\[
D_{KL}(p||q) = -\frac{1}{2} \left[ 1 - \mu^2 - \sigma^2 - \log(\sigma^2) \right]
\]</span></p>
<p>At first it might not make sense to fix <span class="math inline">\(q\)</span> as it is the approximate distribution, fitting a distribution using KL divergence has two modes depending on which argument is the true distribuion. If <span class="math inline">\(p\)</span> is the true distribution, then the fitted distribution will become <em>mode seeking</em>, that is, it will try to fit to a peak of the distribution. If <span class="math inline">\(q\)</span> is the true distribution, then the fitted distribution will become <em>mean seeking</em>, that is, it will try to adjust itself to have the same mean as the distribution. Since for a normal the mean is the same as the mode, the two modes are equivalent. Also, fixing <span class="math inline">\(q\)</span> results in a nice closed form solution for the KL-Divergence.</p>
<hr>
<p>For now these are all the loss and regularization functions we will be using. A couple more will be introduced when we discuss Decision Trees (CART) as a basic understanding of them is needed to motivate the repective losses.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>